{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Compression using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "#prepare dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trainX, trainY, testX, testY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Architecture:\n",
    "\n",
    "5 hidden layers with 1025 units in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024) \n",
      " (?, 1024) \n",
      " (?, 1024) \n",
      " (?, 1024) \n",
      " (?, 1024) \n",
      " (?, 10) \n",
      "\n",
      "(784, 1024) \n",
      " (1024, 1024) \n",
      " (1024, 1024) \n",
      " (1024, 1024) \n",
      " (1024, 1024) \n",
      " (1024, 10) \n",
      "\n",
      "<tf.Variable 'W1:0' shape=(784, 1024) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  \n",
    "input_units = 784\n",
    "hidden_units = 1024\n",
    "output_units = 10\n",
    "\n",
    "#input placeholder\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "\n",
    "#hidden layers 1-5\n",
    "w1 = tf.get_variable(\"W1\", shape=[input_units, hidden_units], initializer=tf.initializers.he_normal())\n",
    "b1 = tf.get_variable(\"b1\", shape=[1,hidden_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.get_variable(\"W2\", shape=[hidden_units, hidden_units], initializer=tf.initializers.he_normal())\n",
    "b2 = tf.get_variable(\"b2\", shape=[1,hidden_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "\n",
    "w3 = tf.get_variable(\"W3\", shape=[hidden_units, hidden_units], initializer=tf.initializers.he_normal())\n",
    "b3 = tf.get_variable(\"b3\", shape=[1,hidden_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "h3 = tf.nn.relu(tf.matmul(h2, w3) + b3)\n",
    "\n",
    "w4 = tf.get_variable(\"W4\", shape=[hidden_units, hidden_units], initializer=tf.initializers.he_normal())\n",
    "b4 = tf.get_variable(\"b4\", shape=[1,hidden_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "h4 = tf.nn.relu(tf.matmul(h3, w4) + b4)\n",
    "\n",
    "w5 = tf.get_variable(\"W5\", shape=[hidden_units, hidden_units], initializer=tf.initializers.he_normal())\n",
    "b5 = tf.get_variable(\"b5\", shape=[1,hidden_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "h5 = tf.nn.relu(tf.matmul(h4, w5) + b5)\n",
    "\n",
    "w6 = tf.get_variable(\"W6\", shape=[hidden_units, output_units], initializer=tf.initializers.he_normal())\n",
    "b6 = tf.get_variable(\"b6\", shape=[1,output_units], initializer=tf.initializers.he_normal())\n",
    "\n",
    "#output\n",
    "y = tf.matmul(h5, w6) + b6\n",
    "\n",
    "#output placeholder\n",
    "y_ = tf.placeholder(tf.int64, [None,10])\n",
    "print(h1.shape,\"\\n\",h2.shape,\"\\n\",h3.shape,\"\\n\",h4.shape,\"\\n\",h5.shape,\"\\n\",y.shape,\"\\n\")\n",
    "print(w1.shape,\"\\n\",w2.shape,\"\\n\",w3.shape,\"\\n\",w4.shape,\"\\n\",w5.shape,\"\\n\",w6.shape,\"\\n\")\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss used for the network is softmax cross entropy. Optimizer is Adam with learning rate of 0.001 and trained over 200 epochs or 98% accuracy, whichever occurs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_, logits=y)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "loss_log = []\n",
    "\n",
    "#intialize tensorflow global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#number of epochs, breaks when accuracy reaches 98\n",
    "epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the network to achieve 98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "accucracy is 90.00999927520752\n",
      "Loss: 0.22595309\n",
      "Epoch: 2\n",
      "accucracy is 92.6800012588501\n",
      "Loss: 0.15739328\n",
      "Epoch: 3\n",
      "accucracy is 93.72000098228455\n",
      "Loss: 0.22399208\n",
      "Epoch: 4\n",
      "accucracy is 95.3000009059906\n",
      "Loss: 0.17944138\n",
      "Epoch: 5\n",
      "accucracy is 95.26000022888184\n",
      "Loss: 0.05124802\n",
      "Epoch: 6\n",
      "accucracy is 95.44000029563904\n",
      "Loss: 0.08696851\n",
      "Epoch: 7\n",
      "accucracy is 95.92999815940857\n",
      "Loss: 0.066148855\n",
      "Epoch: 8\n",
      "accucracy is 96.50999903678894\n",
      "Loss: 0.09461052\n",
      "Epoch: 9\n",
      "accucracy is 96.75999879837036\n",
      "Loss: 0.077253826\n",
      "Epoch: 10\n",
      "accucracy is 96.99000120162964\n",
      "Loss: 0.067374304\n",
      "Epoch: 11\n",
      "accucracy is 97.079998254776\n",
      "Loss: 0.13215718\n",
      "Epoch: 12\n",
      "accucracy is 97.04999923706055\n",
      "Loss: 0.028826457\n",
      "Epoch: 13\n",
      "accucracy is 96.92000150680542\n",
      "Loss: 0.06229269\n",
      "Epoch: 14\n",
      "accucracy is 97.22999930381775\n",
      "Loss: 0.10672052\n",
      "Epoch: 15\n",
      "accucracy is 96.34000062942505\n",
      "Loss: 0.05290363\n",
      "Epoch: 16\n",
      "accucracy is 96.70000076293945\n",
      "Loss: 0.021453135\n",
      "Epoch: 17\n",
      "accucracy is 97.4399983882904\n",
      "Loss: 0.031099897\n",
      "Epoch: 18\n",
      "accucracy is 96.61999940872192\n",
      "Loss: 0.11563498\n",
      "Epoch: 19\n",
      "accucracy is 97.36999869346619\n",
      "Loss: 0.062069837\n",
      "Epoch: 20\n",
      "accucracy is 96.81000113487244\n",
      "Loss: 0.039467838\n",
      "Epoch: 21\n",
      "accucracy is 97.3800003528595\n",
      "Loss: 0.029903395\n",
      "Epoch: 22\n",
      "accucracy is 97.33999967575073\n",
      "Loss: 0.039017025\n",
      "Epoch: 23\n",
      "accucracy is 96.82999849319458\n",
      "Loss: 0.081273034\n",
      "Epoch: 24\n",
      "accucracy is 97.2100019454956\n",
      "Loss: 0.03674303\n",
      "Epoch: 25\n",
      "accucracy is 97.10000157356262\n",
      "Loss: 0.032626934\n",
      "Epoch: 26\n",
      "accucracy is 97.75999784469604\n",
      "Loss: 0.009252205\n",
      "Epoch: 27\n",
      "accucracy is 97.28000164031982\n",
      "Loss: 0.0760306\n",
      "Epoch: 28\n",
      "accucracy is 97.85000085830688\n",
      "Loss: 0.02938898\n",
      "Epoch: 29\n",
      "accucracy is 97.18000292778015\n",
      "Loss: 0.025104567\n",
      "Epoch: 30\n",
      "accucracy is 97.71999716758728\n",
      "Loss: 0.0040466036\n",
      "Epoch: 31\n",
      "accucracy is 97.2100019454956\n",
      "Loss: 0.035367645\n",
      "Epoch: 32\n",
      "accucracy is 97.75000214576721\n",
      "Loss: 0.099022046\n",
      "Epoch: 33\n",
      "accucracy is 97.50000238418579\n",
      "Loss: 0.19254464\n",
      "Epoch: 34\n",
      "accucracy is 97.42000102996826\n",
      "Loss: 0.009722525\n",
      "Epoch: 35\n",
      "accucracy is 97.79999852180481\n",
      "Loss: 0.031946514\n",
      "Epoch: 36\n",
      "accucracy is 97.60000109672546\n",
      "Loss: 0.032956604\n",
      "Epoch: 37\n",
      "accucracy is 97.83999919891357\n",
      "Loss: 0.03732387\n",
      "Epoch: 38\n",
      "accucracy is 97.42000102996826\n",
      "Loss: 0.03256196\n",
      "Epoch: 39\n",
      "accucracy is 97.69999980926514\n",
      "Loss: 0.06860974\n",
      "Epoch: 40\n",
      "accucracy is 97.72999882698059\n",
      "Loss: 0.0025842136\n",
      "Epoch: 41\n",
      "accucracy is 97.89000153541565\n",
      "Loss: 0.008177463\n",
      "Epoch: 42\n",
      "accucracy is 98.15000295639038\n",
      "Loss: 0.0035329931\n"
     ]
    }
   ],
   "source": [
    "sess_a3_0 = tf.Session()\n",
    "sess_a3_0.run(init)\n",
    "loss_log=[]\n",
    "#batch size\n",
    "batches=int(len(mnist.train.labels)/550)\n",
    "epoch_plot=[]\n",
    "#train model\n",
    "for epoch in range(epochs):\n",
    "    epoch_plot.append(epoch+1)\n",
    "    for _ in range(batches):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        loss,_ = sess_a3_0.run([cross_entropy,train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    loss_log.append(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_result = sess_a3_0.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "    print(\"Epoch:\",epoch + 1)\n",
    "    print(\"accucracy is\",accuracy_result*100)\n",
    "    print(\"Loss:\",loss)\n",
    "    if accuracy_result*100>98:\n",
    "        break\n",
    "    \n",
    "# # Test trained model on training dataset  \n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# accuracy_result = sess_a3_1.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "# print(\"accucracy is\",accuracy_result*100)\n",
    "#sess_a3_1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate SVD on weights of layer 1-5 using tf.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,) (784, 784) (1024, 784)\n"
     ]
    }
   ],
   "source": [
    "s1, u1, v1 = sess_a3_0.run(tf.svd(w1))\n",
    "s2, u2, v2 = sess_a3_0.run(tf.svd(w2))\n",
    "s3, u3, v3 = sess_a3_0.run(tf.svd(w3))\n",
    "s4, u4, v4 = sess_a3_0.run(tf.svd(w4))\n",
    "s5, u5, v5 = sess_a3_0.run(tf.svd(w5))\n",
    "print(s1.shape,u1.shape,v1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving weight of output layer and biases of all layers to be used in the second neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w6_orig = sess_a3_0.run(w6)\n",
    "b1_orig = sess_a3_0.run(b1)\n",
    "b2_orig = sess_a3_0.run(b2)\n",
    "b3_orig = sess_a3_0.run(b3)\n",
    "b4_orig = sess_a3_0.run(b4)\n",
    "b5_orig = sess_a3_0.run(b5)\n",
    "b6_orig = sess_a3_0.run(b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5\n",
    "\n",
    "Calculating accuracy of the model with varying sizes from 10...784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4076, 0.8475, 0.9758, 0.9785, 0.9805, 0.9818]\n"
     ]
    }
   ],
   "source": [
    "D = [10,20,50,100,200,784]\n",
    "acc=[]\n",
    "for i in D:\n",
    "    w1_1 = tf.matmul(tf.multiply(s1[:i,], u1[:,:i]),tf.transpose(v1[:,:i]))\n",
    "\n",
    "    h1_1 = tf.nn.relu(tf.matmul(x, w1_1) + b1)\n",
    "\n",
    "    w2_1 = tf.matmul(tf.multiply(s2[:i,], u2[:,:i]),tf.transpose(v2[:,:i]))\n",
    "\n",
    "    h2_1 = tf.nn.relu(tf.matmul(h1_1, w2_1) + b2)\n",
    "\n",
    "    w3_1 = tf.matmul(tf.multiply(s3[:i,], u3[:,:i]),tf.transpose(v3[:,:i]))\n",
    "\n",
    "    h3_1 = tf.nn.relu(tf.matmul(h2_1, w3_1) + b3)\n",
    "\n",
    "    w4_1 = tf.matmul(tf.multiply(s4[:i,], u4[:,:i]),tf.transpose(v4[:,:i]))\n",
    "\n",
    "    h4_1 = tf.nn.relu(tf.matmul(h3_1, w4_1) + b4)\n",
    "\n",
    "    w5_1 = tf.matmul(tf.multiply(s5[:i,], u5[:,:i]),tf.transpose(v5[:,:i]))\n",
    "\n",
    "    h5_1 = tf.nn.relu(tf.matmul(h4_1, w5_1) + b5)\n",
    "\n",
    "    y1 = tf.matmul(h5_1, w6_orig) + b6_orig\n",
    "    correct_prediction1 = tf.equal(tf.argmax(y1, 1),tf.argmax(y_,1))\n",
    "    accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))\n",
    "\n",
    "    acc.append(sess_a3_0.run(accuracy1, feed_dict={x: mnist.test.images,y_: mnist.test.labels}))\n",
    "    #print(\"D:\",i,acc)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graph of D vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(D,acc)\n",
    "plt.xlabel('Size of D')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the first session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_a3_0.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Part a)\n",
    "\n",
    "Setup second network using weights acquired from SVD and D=20.\n",
    "\n",
    "Hidden layers 1-5 using weights from SVD and bias learned from the first network.\n",
    "\n",
    "The output layers uses weights and biases from the first network.\n",
    "\n",
    "Since the weights are already initialized with the learnt weights from the first network, the accuracy starts off at 96% and reaches ~97% because of D=20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "accucracy is 96.35999798774719\n",
      "Loss: 0.13106595\n",
      "Epoch:  1\n",
      "accucracy is 96.78999781608582\n",
      "Loss: 0.07071133\n",
      "Epoch:  2\n",
      "accucracy is 96.88000082969666\n",
      "Loss: 0.06326151\n",
      "Epoch:  3\n",
      "accucracy is 96.97999954223633\n",
      "Loss: 0.113481544\n",
      "Epoch:  4\n",
      "accucracy is 97.26999998092651\n",
      "Loss: 0.14831045\n",
      "Epoch:  5\n",
      "accucracy is 97.11999893188477\n",
      "Loss: 0.09478518\n",
      "Epoch:  6\n",
      "accucracy is 97.07000255584717\n",
      "Loss: 0.17179197\n",
      "Epoch:  7\n",
      "accucracy is 97.18000292778015\n",
      "Loss: 0.011497281\n",
      "Epoch:  8\n",
      "accucracy is 97.26999998092651\n",
      "Loss: 0.11837897\n",
      "Epoch:  9\n",
      "accucracy is 97.2100019454956\n",
      "Loss: 0.0877135\n",
      "Epoch:  10\n",
      "accucracy is 97.32000231742859\n",
      "Loss: 0.062031258\n",
      "Epoch:  11\n",
      "accucracy is 97.24000096321106\n",
      "Loss: 0.13074447\n",
      "Epoch:  12\n",
      "accucracy is 97.29999899864197\n",
      "Loss: 0.12794489\n",
      "Epoch:  13\n",
      "accucracy is 97.3800003528595\n",
      "Loss: 0.026732272\n",
      "Epoch:  14\n",
      "accucracy is 97.35999703407288\n",
      "Loss: 0.08853692\n",
      "Epoch:  15\n",
      "accucracy is 97.32999801635742\n",
      "Loss: 0.05002929\n",
      "Epoch:  16\n",
      "accucracy is 97.3800003528595\n",
      "Loss: 0.06619106\n",
      "Epoch:  17\n",
      "accucracy is 97.39999771118164\n",
      "Loss: 0.027910167\n",
      "Epoch:  18\n",
      "accucracy is 97.36999869346619\n",
      "Loss: 0.043866716\n",
      "Epoch:  19\n",
      "accucracy is 97.50999808311462\n",
      "Loss: 0.038907003\n",
      "Epoch:  20\n",
      "accucracy is 97.36999869346619\n",
      "Loss: 0.026386142\n",
      "Epoch:  21\n",
      "accucracy is 97.51999974250793\n",
      "Loss: 0.03384667\n",
      "Epoch:  22\n",
      "accucracy is 97.57000207901001\n",
      "Loss: 0.05315028\n",
      "Epoch:  23\n",
      "accucracy is 97.63000011444092\n",
      "Loss: 0.026709303\n",
      "Epoch:  24\n",
      "accucracy is 97.5600004196167\n",
      "Loss: 0.055364836\n",
      "Epoch:  25\n",
      "accucracy is 97.42000102996826\n",
      "Loss: 0.12870374\n",
      "Epoch:  26\n",
      "accucracy is 97.60000109672546\n",
      "Loss: 0.02459458\n",
      "Epoch:  27\n",
      "accucracy is 97.39000201225281\n",
      "Loss: 0.042608514\n",
      "Epoch:  28\n",
      "accucracy is 97.7400004863739\n",
      "Loss: 0.033615626\n",
      "Epoch:  29\n",
      "accucracy is 97.45000004768372\n",
      "Loss: 0.007466965\n",
      "Epoch:  30\n",
      "accucracy is 97.61999845504761\n",
      "Loss: 0.097750455\n",
      "Epoch:  31\n",
      "accucracy is 97.57000207901001\n",
      "Loss: 0.015917916\n",
      "Epoch:  32\n",
      "accucracy is 97.65999913215637\n",
      "Loss: 0.029762302\n",
      "Epoch:  33\n",
      "accucracy is 97.68999814987183\n",
      "Loss: 0.05844574\n",
      "Epoch:  34\n",
      "accucracy is 97.71999716758728\n",
      "Loss: 0.030289257\n",
      "Epoch:  35\n",
      "accucracy is 97.65999913215637\n",
      "Loss: 0.02010381\n",
      "Epoch:  36\n",
      "accucracy is 97.63000011444092\n",
      "Loss: 0.008854239\n",
      "Epoch:  37\n",
      "accucracy is 97.71000146865845\n",
      "Loss: 0.037754454\n",
      "Epoch:  38\n",
      "accucracy is 97.57999777793884\n",
      "Loss: 0.043549493\n",
      "Epoch:  39\n",
      "accucracy is 97.69999980926514\n",
      "Loss: 0.0769767\n",
      "Epoch:  40\n",
      "accucracy is 97.53000140190125\n",
      "Loss: 0.045526687\n",
      "Epoch:  41\n",
      "accucracy is 97.71000146865845\n",
      "Loss: 0.022651114\n",
      "Epoch:  42\n",
      "accucracy is 97.83999919891357\n",
      "Loss: 0.018806813\n",
      "Epoch:  43\n",
      "accucracy is 97.86999821662903\n",
      "Loss: 0.026063979\n",
      "Epoch:  44\n",
      "accucracy is 97.67000079154968\n",
      "Loss: 0.028657284\n",
      "Epoch:  45\n",
      "accucracy is 97.78000116348267\n",
      "Loss: 0.02640372\n",
      "Epoch:  46\n",
      "accucracy is 97.680002450943\n",
      "Loss: 0.02567407\n",
      "Epoch:  47\n",
      "accucracy is 97.53000140190125\n",
      "Loss: 0.06610858\n",
      "Epoch:  48\n",
      "accucracy is 97.79000282287598\n",
      "Loss: 0.054580588\n",
      "Epoch:  49\n",
      "accucracy is 97.79000282287598\n",
      "Loss: 0.0068250964\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  \n",
    "\n",
    "#input placeholder\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "\n",
    "#layer 1\n",
    "u1_1 = tf.Variable(u1[:,:20])\n",
    "v_hat1 = tf.Variable(tf.multiply(s1[:20], v1[:,:20])) \n",
    "h1_2 = tf.nn.relu(tf.matmul(x, tf.matmul(u1_1,tf.transpose(v_hat1))) + b1_orig)\n",
    "\n",
    "#layer 2\n",
    "u2_1 = tf.Variable(u2[:,:20])\n",
    "v_hat2 = tf.Variable(tf.multiply(s2[:20], v2[:,:20]))\n",
    "h2_2 = tf.nn.relu(tf.matmul(h1_2, tf.matmul(u2_1,tf.transpose(v_hat2))) + b2_orig)\n",
    "\n",
    "#layer 3\n",
    "u3_1 = tf.Variable(u3[:,:20])\n",
    "v_hat3 = tf.Variable(tf.multiply(s3[:20], v3[:,:20])) \n",
    "h3_2 = tf.nn.relu(tf.matmul(h2_2, tf.matmul(u3_1,tf.transpose(v_hat3))) + b3_orig)\n",
    "\n",
    "#layer 4\n",
    "u4_1 = tf.Variable(u4[:,:20])\n",
    "v_hat4 = tf.Variable(tf.multiply(s4[:20], v4[:,:20])) \n",
    "h4_2 = tf.nn.relu(tf.matmul(h3_2, tf.matmul(u4_1,tf.transpose(v_hat4))) + b4_orig)\n",
    "\n",
    "#layer 5\n",
    "u5_1 = tf.Variable(u5[:,:20])\n",
    "v_hat5 = tf.Variable(tf.multiply(s5[:20], v5[:,:20])) \n",
    "h5_2 = tf.nn.relu(tf.matmul(h4_2, tf.matmul(u5_1,tf.transpose(v_hat5))) + b5_orig)\n",
    "\n",
    "#output layer \n",
    "y = tf.matmul(h5_2, w6_orig) + b6_orig\n",
    "y_ = tf.placeholder(tf.int64, [None,10])\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_, logits=y)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "loss_log = []\n",
    "\n",
    "#intialize tensorflow global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#number of epochs, breaks when accuracy reaches 98\n",
    "epochs=50\n",
    "\n",
    "#run new network\n",
    "sess_a3_1 = tf.Session()\n",
    "sess_a3_1.run(init)\n",
    "\n",
    "\n",
    "#batch size\n",
    "batches=int(len(mnist.train.labels)/1000)\n",
    "epoch_plot=[]\n",
    "#train model\n",
    "for epoch in range(epochs):\n",
    "    epoch_plot.append(epoch)\n",
    "    print(\"Epoch: \",epoch)\n",
    "    for _ in range(batches):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        loss,_ = sess_a3_1.run([cross_entropy,train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    loss_log.append(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_result = sess_a3_1.run(accuracy, feed_dict={x: mnist.test.images,y_: mnist.test.labels})\n",
    "    print(\"accucracy is\",accuracy_result*100)\n",
    "    print(\"Loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close second session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_a3_1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
